=======================================================================
NOCpulse Scheduler for High Availability and Fault Tolerence (NP SHAFT)
=======================================================================


Overview
--------

The NOCpulse SHAFT(tm) is the satellite scheduler system responsible
for scheduling, spawning, reaping, and timing probe execution.



Architecture
------------
The NOCpulse SHAFT is composed of two subsystems, the Scheduler and
the Kernel, plus tools for starting and monitoring the system and
for loading jobs.

The Scheduler is responsible for maintaining job queues and dispatching 
jobs, and for intra-job communication.  It's implemented (at the top level) 
by the NOCpulse::Scheduler Perl module.

The Kernel is responsible for actually running the jobs (i.e. taking
dispatched jobs from the Scheduler, spawning them, reaping them, and
returning them to the queue, as well as runtime error handling).  It's
implemented by kernel.pl, using the NOCpulse::ProcessPool Perl module.

Under the high-availability architecture, the Kernel and the Scheduler
will run on different hosts.  (That is, in a pool of N satellites, all
N will run a Kernel instance but only one will run a Scheduler instance.)




Details
-------

  * Scheduler

    The Scheduler maintains two lists of jobs:  those currently running
    (the "run queue") and those waiting to run (the "ready queue").
    When a Kernel requests a job from the scheduler, it pulls a job from
    the ready queue (based on last execution time, priority, etc) and
    places a copy to the run queue, and passes another copy to the Kernel.
    After the job has run, the Kernel passes it back to the Scheduler
    with a new target execution time.

    The Scheduler also maintains a MessageDictionary, a kind of 
    switchboard for inter-job messages.  During a run, a job may
    elect to send a message to another job; when the run is complete 
    and the job is passed back to the Scheduler, any outbound 
    messages are moved to the switchboard.  When the other job is
    dispatched, messages from the switchboard are made available to 
    it.  (This is the mechanism by which a parent probe lets its
    children know when it has failed, for instance.)
    

  * Kernel
    
    At present, the Kernel drives the SHAFT.  When started by an init
    script, it reads in the currently configured jobs and passes them
    to the Scheduler.  Then it enters a daemon loop where it does the
    following:

      - Maintains an N-process pool (for configurable N) of running jobs 
        (as long as there are jobs ready to run) by requesting new jobs
	from the Scheduler;

      - Reaps finished jobs and returns them to the Scheduler;

      - Kills "runaway" jobs (jobs that live longer than the max
        allowed lifetime), invoking their timeout handlers;

      - Loads new configuration when available (see below);

      - Rotates logfiles as appropriate.


  * scheduleEvents

    Last but not least, scheduleEvents is an external Perl program that
    fetches probe configuration from the configuration database (via the
    satconfig VIP) and generates a list of jobs for the Scheduler.  It
    uses a flag file to notify the running Kernel that a new configuration
    is available.  In general, scheduleEvents is run remotely via SputLite
    by clicking on the "Configure Satellites" button in the GUI.
    
=======================================================================
